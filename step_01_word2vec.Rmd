---
title: "H2O NLP Demo - Step 1: Word2Vec"
author: "Jo-fai (Joe) Chow"
date: "28/03/2018"
output: 
  html_document: 
    df_print: kable
    fig_height: 7
    fig_width: 14
    highlight: tango
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 2
  html_notebook: 
    fig_height: 7
    fig_width: 14
    highlight: tango
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

# Libraries

```{r}
suppressPackageStartupMessages(library(h2o))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(caret))
```

<br>

# Raw Data

```{r}
# Import CSV (note: link in email)
d_text = fread("./model_data_no_embeddings.csv", encoding = "Latin-1")
```

```{r}
# Look at genre
table(d_text$genre)
```

```{r}
# Look at one sample
d_text[1,]
```

<br>


# Basic Text Cleaning

**Note**: It is important to remove `\n` -- it appears to cause a parsing error when converting to an `H2OFrame`

<br>

## Remove Special Characters

**Reference**: https://stackoverflow.com/questions/10294284/remove-all-special-characters-from-a-string-in-r

```{r}
d_text[, title := str_replace_all(title, "[[:punct:]]", "")]
d_text[, plot := str_replace_all(plot, "[[:punct:]]", "")]
d_text[1,]
```

<br>

## Split Data into Training/Testing

```{r}
set.seed(54321)
row_train = createDataPartition(y = d_text$genre, p = 0.75, list = FALSE)
d_train = d_text[row_train, ]
d_test = d_text[-row_train, ]
```

<br>

```{r}
# Look at genre in d_train
table(d_train$genre)
```

```{r}
# Look at genre in d_test
table(d_test$genre)
```

<br>

# Using H2O Word2Vec to Transform Words to Vectors


## Start H2O

```{r}
h2o.init(nthreads = -1, max_mem_size = "8g")
h2o.removeAll() # clean up
h2o.no_progress() # disable progress bar
```

<br>

## H2OFrame

```{r}
h_train = as.h2o(d_train, destination_frame = "h_train")
h_test = as.h2o(d_test, destination_frame = "h_test")
```

<br>


## Define Stop Words

```{r}
stop_words =
  c("ax","i","you","edu","s","t","m","subject","can","lines","re","what",
    "there","all","we","one","the","a","an","of","or","in","for","by","on",
    "but","is","in","a","not","with","as","was","if","they","are","this","and",
    "it","have", "from","at","my","be","by","not","that","to","from","com",
    "org","like","likes","so", "hashtag")
```

<br>


## A Helper Function to Tokenize Text

```{r}
tokenize_helper = function(text, stop_words) {
  
  # Main H2O Tokenize function
  h_words = h2o.tokenize(text, split = "\\\\W+")
  
  # convert to lower case
  h_words <- h2o.tolower(h_words)
  
  #  Remove Short Words (< 2 Characters)
  h_words_length = h2o.nchar(h_words)
  h_words = h_words[is.na(h_words_length) || h_words_length >= 2,]
  
  # Remove Words that Contain Numbers
  h_words = h_words[h2o.grep("[0-9]", h_words, invert = TRUE, output.logical = TRUE),]
  
  # Remove Stop Words
  h_words = h_words[is.na(h_words) || (! h_words %in% stop_words), ]
  
  # Return
  return(h_words)
  
}
```

<br>

## Tokenize 

```{r}
h_words_train = tokenize_helper(h_train$plot, stop_words)
head(h_words_train)
dim(h_words_train)
```

<br>


## Build Word2Vec Model

### Quick Run

**Notes**: Results are not great.


```{r}
model_w2v_quick = 
  h2o.word2vec(training_frame = h_words_train,
               model_id = "model_quick",
               min_word_freq = 5,
               word_model = "SkipGram",
               norm_model = "HSM",
               vec_size = 10,
               window_size = 5,
               sent_sample_rate = 0.001,
               init_learning_rate = 0.025,
               epochs = 1,
               max_runtime_secs = 60)
```

```{r}
# Sanity check - find synonyms for a few common words
print(h2o.findSynonyms(model_w2v_quick, "king", count = 5))
print(h2o.findSynonyms(model_w2v_quick, "water", count = 5))
```

<br>

### Normal Run

**Notes**: Results are ok but it takes a few minutes to train.

```{r}
model_w2v_normal =
  h2o.word2vec(training_frame = h_words_train,
               model_id = "model_normal",
               min_word_freq = 5,
               word_model = "SkipGram",
               norm_model = "HSM",
               vec_size = 100,
               window_size = 5,
               sent_sample_rate = 0.001,
               init_learning_rate = 0.025,
               epochs = 5,
               max_runtime_secs = 600)
```

```{r}
# Sanity check - find synonyms for a few common words
print(h2o.findSynonyms(model_w2v_normal, "king", count = 5))
print(h2o.findSynonyms(model_w2v_normal, "water", count = 5))
```

<br>

### Using a Pre-trained Model from Internet

**Reference**: 

- https://nlp.stanford.edu/projects/glove/
- https://stackoverflow.com/questions/42982176/does-or-will-h2o-provide-any-pretrained-vectors-for-use-with-h2o-word2vec

**Notes**: Results are great.

<br>

```{r}
# Download this from http://nlp.stanford.edu/data/glove.6B.zip
h_pretrained = h2o.importFile("./glove.6B.300d.txt")
```

```{r}
model_w2v_pretrained =
  h2o.word2vec(pre_trained = h_pretrained,
               model_id = "model_pretrained",
               vec_size = 300)
```
  
```{r}
# Sanity check - find synonyms for a few common words
print(h2o.findSynonyms(model_w2v_pretrained, "king", count = 5))
print(h2o.findSynonyms(model_w2v_pretrained, "water", count = 5))
```

<br>

## Save Word2Vec Models (for Transformation in the Future)

```{r}
h2o.saveModel(model_w2v_quick, path = "./", force = TRUE)
h2o.saveModel(model_w2v_normal, path = "./", force = TRUE)
h2o.saveModel(model_w2v_pretrained, path = "./", force = TRUE)
```

<br>


## Using Word2Vec Model

**Notes**: Using the best model (e.g. `model_w2v_pretrained`).

```{r}
# Transform Words in Training Set
h_vec_train = h2o.transform(word2vec = model_w2v_pretrained,
                            words = h_words_train,
                            aggregate_method = "AVERAGE")
head(h_vec_train[1:5, 1:5])
dim(h_vec_train)
```

<br>

```{r}
# Transform Words in Test Set
h_words_test = tokenize_helper(h_test$plot, stop_words)

h_vec_test = h2o.transform(word2vec = model_w2v_pretrained,
                           words = h_words_test,
                           aggregate_method = "AVERAGE")
head(h_vec_test[1:5, 1:5])
dim(h_vec_test)
```

<br>

## Final Output

```{r}
# Combine original data and new vectors
h_train_final = h2o.cbind(h_train, h_vec_train)
h_test_final = h2o.cbind(h_test, h_vec_test)
```

<br>

```{r}
print(h_train_final[1,])
```

<br>

```{r}
# Export to CSV
h2o.exportFile(h_train_final, path = "./train.csv", force = TRUE)
h2o.exportFile(h_test_final, path = "./test.csv", force = TRUE)
```



